# CIFAR100 + WideResNet28-10 with this config

pruners:
  pruner_1:
    class: UnstructuredMagnitudePruner
    epochs: [10, 33, 180]
    initial_sparsity: 0.75
    target_sparsity: 0.75
    weight_only: True
    modules: [conv1, layer1.0.conv1, layer1.0.conv2, layer1.0.shortcut.0, layer1.1.conv1, layer1.1.conv2,
              layer1.2.conv1, layer1.2.conv2, layer1.3.conv1, layer1.3.conv2, layer2.0.conv1, layer2.0.conv2,
              layer2.0.shortcut.0, layer2.1.conv1, layer2.1.conv2, layer2.2.conv1, layer2.2.conv2, layer2.3.conv1, 
              layer2.3.conv2, layer3.0.conv1, layer3.0.conv2, layer3.0.shortcut.0, layer3.1.conv1, layer3.1.conv2,
              layer3.2.conv1, layer3.2.conv2, layer3.3.conv1, layer3.3.conv2, linear]
    keep_pruned: False

recyclers:
  recycler_1:
    class: RestoreWeights
    weight_only: True
    epochs: [30, 33, 170] # [start, freq, end] for now (TODO: but can extend functionality?)
    modules: [conv1, layer1.0.conv1, layer1.0.conv2, layer1.0.shortcut.0, layer1.1.conv1, layer1.1.conv2,
              layer1.2.conv1, layer1.2.conv2, layer1.3.conv1, layer1.3.conv2, layer2.0.conv1, layer2.0.conv2,
              layer2.0.shortcut.0, layer2.1.conv1, layer2.1.conv2, layer2.2.conv1, layer2.2.conv2, layer2.3.conv1, 
              layer2.3.conv2, layer3.0.conv1, layer3.0.conv2, layer3.0.shortcut.0, layer3.1.conv1, layer3.1.conv2,
              layer3.2.conv1, layer3.2.conv2, layer3.3.conv1, layer3.3.conv2, linear]
 
trainers:
  # use this trainer name unless you want KD or other custom thing
  default_trainer:
    optimizer:
      class: SGD
      lr: 0.08
      momentum: 0.9
      weight_decay: 0.0005
      nesterov: True

    lr_scheduler:
      class: ExponentialLR
      gamma: 0.1
      epochs: [75, 40, 170]
